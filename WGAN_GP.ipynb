{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "name": "WGAN-GP.ipynb",
      "provenance": [],
      "collapsed_sections": [],
      "include_colab_link": true
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "accelerator": "GPU"
  },
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github",
        "colab_type": "text"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/michaelhasey/GAN-ITECTURE/blob/master/WGAN_GP.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "WyOtirrtoYRQ",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "# import Libraries\n",
        "%matplotlib inline\n",
        "\n",
        "import os\n",
        "import matplotlib.pyplot as plt\n",
        "\n",
        "# from models.WGANGP import WGANGP\n",
        "# from utils.loaders import load_celeb\n",
        "import pickle\n",
        "from keras.preprocessing.image import ImageDataGenerator, load_img, save_img, img_to_array\n",
        "\n"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "J7iMNWZStTM1",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "from google.colab import drive\n",
        "drive.mount('/content/gdrive')\n"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "O5tQW3shqxOy",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "# # ('./gdrive/My Drive/datasets/dataset_neoclassical_large_test/)\n",
        "def load_celeb(data_name, image_size, batch_size):\n",
        "    data_folder = os.path.join(\"./gdrive/My Drive/datasets/\", data_name)\n",
        "\n",
        "    data_gen = ImageDataGenerator(preprocessing_function=lambda x: (x.astype('float32') - 127.5) / 127.5)\n",
        "\n",
        "    x_train = data_gen.flow_from_directory(data_folder\n",
        "                                            , target_size = (image_size,image_size)\n",
        "                                            , batch_size = batch_size\n",
        "                                            , shuffle = True\n",
        "                                            , class_mode = 'input'\n",
        "                                            , subset = \"training\"\n",
        "                                                )\n",
        "\n",
        "    return x_train"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "7k_MRJRTqUZ5",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "# WGANGP\n",
        "\n",
        "from keras.layers import Input, Conv2D, Flatten, Dense, Conv2DTranspose, Reshape, Lambda, Activation, BatchNormalization, LeakyReLU, Dropout, ZeroPadding2D, UpSampling2D\n",
        "from keras.layers.merge import _Merge\n",
        "\n",
        "from keras.models import Model, Sequential\n",
        "from keras import backend as K\n",
        "from keras.optimizers import Adam, RMSprop\n",
        "from keras.callbacks import ModelCheckpoint \n",
        "from keras.utils import plot_model\n",
        "from keras.initializers import RandomNormal\n",
        "\n",
        "from functools import partial\n",
        "\n",
        "import numpy as np\n",
        "import json\n",
        "import os\n",
        "import pickle\n",
        "import matplotlib.pyplot as plt\n",
        "\n",
        "\n",
        "class RandomWeightedAverage(_Merge):\n",
        "    def __init__(self, batch_size):\n",
        "        super().__init__()\n",
        "        self.batch_size = batch_size\n",
        "    \"\"\"Provides a (random) weighted average between real and generated image samples\"\"\"\n",
        "    def _merge_function(self, inputs):\n",
        "        alpha = K.random_uniform((self.batch_size, 1, 1, 1))\n",
        "        return (alpha * inputs[0]) + ((1 - alpha) * inputs[1])\n",
        "\n",
        "class WGANGP():\n",
        "    def __init__(self\n",
        "        , input_dim\n",
        "        , critic_conv_filters\n",
        "        , critic_conv_kernel_size\n",
        "        , critic_conv_strides\n",
        "        , critic_batch_norm_momentum\n",
        "        , critic_activation\n",
        "        , critic_dropout_rate\n",
        "        , critic_learning_rate\n",
        "        , generator_initial_dense_layer_size\n",
        "        , generator_upsample\n",
        "        , generator_conv_filters\n",
        "        , generator_conv_kernel_size\n",
        "        , generator_conv_strides\n",
        "        , generator_batch_norm_momentum\n",
        "        , generator_activation\n",
        "        , generator_dropout_rate\n",
        "        , generator_learning_rate\n",
        "        , optimiser\n",
        "        , grad_weight\n",
        "        , z_dim\n",
        "        , batch_size\n",
        "        ):\n",
        "\n",
        "        self.name = 'gan'\n",
        "\n",
        "        self.input_dim = input_dim\n",
        "        self.critic_conv_filters = critic_conv_filters\n",
        "        self.critic_conv_kernel_size = critic_conv_kernel_size\n",
        "        self.critic_conv_strides = critic_conv_strides\n",
        "        self.critic_batch_norm_momentum = critic_batch_norm_momentum\n",
        "        self.critic_activation = critic_activation\n",
        "        self.critic_dropout_rate = critic_dropout_rate\n",
        "        self.critic_learning_rate = critic_learning_rate\n",
        "\n",
        "        self.generator_initial_dense_layer_size = generator_initial_dense_layer_size\n",
        "        self.generator_upsample = generator_upsample\n",
        "        self.generator_conv_filters = generator_conv_filters\n",
        "        self.generator_conv_kernel_size = generator_conv_kernel_size\n",
        "        self.generator_conv_strides = generator_conv_strides\n",
        "        self.generator_batch_norm_momentum = generator_batch_norm_momentum\n",
        "        self.generator_activation = generator_activation\n",
        "        self.generator_dropout_rate = generator_dropout_rate\n",
        "        self.generator_learning_rate = generator_learning_rate\n",
        "        \n",
        "        self.optimiser = optimiser\n",
        "\n",
        "        self.z_dim = z_dim\n",
        "\n",
        "        self.n_layers_critic = len(critic_conv_filters)\n",
        "        self.n_layers_generator = len(generator_conv_filters)\n",
        "\n",
        "        self.weight_init = RandomNormal(mean=0., stddev=0.02) #Â 'he_normal' #RandomNormal(mean=0., stddev=0.02)\n",
        "        self.grad_weight = grad_weight\n",
        "        self.batch_size = batch_size\n",
        "\n",
        "\n",
        "        self.d_losses = []\n",
        "        self.g_losses = []\n",
        "        self.epoch = 0\n",
        "\n",
        "        self._build_critic()\n",
        "        self._build_generator()\n",
        "\n",
        "        self._build_adversarial()\n",
        "\n",
        "    def gradient_penalty_loss(self, y_true, y_pred, interpolated_samples):\n",
        "        \"\"\"\n",
        "        Computes gradient penalty based on prediction and weighted real / fake samples\n",
        "        \"\"\"\n",
        "        gradients = K.gradients(y_pred, interpolated_samples)[0]\n",
        "\n",
        "        # compute the euclidean norm by squaring ...\n",
        "        gradients_sqr = K.square(gradients)\n",
        "        #   ... summing over the rows ...\n",
        "        gradients_sqr_sum = K.sum(gradients_sqr,\n",
        "                                  axis=np.arange(1, len(gradients_sqr.shape)))\n",
        "        #   ... and sqrt\n",
        "        gradient_l2_norm = K.sqrt(gradients_sqr_sum)\n",
        "        # compute lambda * (1 - ||grad||)^2 still for each single sample\n",
        "        gradient_penalty = K.square(1 - gradient_l2_norm)\n",
        "        # return the mean as loss over all the batch samples\n",
        "        return K.mean(gradient_penalty)\n",
        "\n",
        "    def wasserstein(self, y_true, y_pred):\n",
        "        return -K.mean(y_true * y_pred)\n",
        "\n",
        "    def get_activation(self, activation):\n",
        "        if activation == 'leaky_relu':\n",
        "            layer = LeakyReLU(alpha = 0.2)\n",
        "        else:\n",
        "            layer = Activation(activation)\n",
        "        return layer\n",
        "\n",
        "    def _build_critic(self):\n",
        "\n",
        "        ### THE critic\n",
        "        critic_input = Input(shape=self.input_dim, name='critic_input')\n",
        "\n",
        "        x = critic_input\n",
        "\n",
        "        for i in range(self.n_layers_critic):\n",
        "\n",
        "            x = Conv2D(\n",
        "                filters = self.critic_conv_filters[i]\n",
        "                , kernel_size = self.critic_conv_kernel_size[i]\n",
        "                , strides = self.critic_conv_strides[i]\n",
        "                , padding = 'same'\n",
        "                , name = 'critic_conv_' + str(i)\n",
        "                , kernel_initializer = self.weight_init\n",
        "                )(x)\n",
        "\n",
        "            if self.critic_batch_norm_momentum and i > 0:\n",
        "                x = BatchNormalization(momentum = self.critic_batch_norm_momentum)(x)\n",
        "\n",
        "            x = self.get_activation(self.critic_activation)(x)\n",
        "\n",
        "            if self.critic_dropout_rate:\n",
        "                x = Dropout(rate = self.critic_dropout_rate)(x)\n",
        "\n",
        "        x = Flatten()(x)\n",
        "\n",
        "        # x = Dense(512, kernel_initializer = self.weight_init)(x)\n",
        "\n",
        "        # x = self.get_activation(self.critic_activation)(x)\n",
        "        \n",
        "        critic_output = Dense(1, activation=None\n",
        "        , kernel_initializer = self.weight_init\n",
        "        )(x)\n",
        "\n",
        "        self.critic = Model(critic_input, critic_output)\n",
        "\n",
        "    def _build_generator(self):\n",
        "\n",
        "        ### THE generator\n",
        "\n",
        "        generator_input = Input(shape=(self.z_dim,), name='generator_input')\n",
        "\n",
        "        x = generator_input\n",
        "\n",
        "        x = Dense(np.prod(self.generator_initial_dense_layer_size), kernel_initializer = self.weight_init)(x)\n",
        "        if self.generator_batch_norm_momentum:\n",
        "            x = BatchNormalization(momentum = self.generator_batch_norm_momentum)(x)\n",
        "        \n",
        "        x = self.get_activation(self.generator_activation)(x)\n",
        "\n",
        "        x = Reshape(self.generator_initial_dense_layer_size)(x)\n",
        "\n",
        "        if self.generator_dropout_rate:\n",
        "            x = Dropout(rate = self.generator_dropout_rate)(x)\n",
        "\n",
        "        for i in range(self.n_layers_generator):\n",
        "\n",
        "            if self.generator_upsample[i] == 2:\n",
        "                x = UpSampling2D()(x)\n",
        "                x = Conv2D(\n",
        "                filters = self.generator_conv_filters[i]\n",
        "                , kernel_size = self.generator_conv_kernel_size[i]\n",
        "                , padding = 'same'\n",
        "                , name = 'generator_conv_' + str(i)\n",
        "                , kernel_initializer = self.weight_init\n",
        "                )(x)\n",
        "            else:\n",
        "\n",
        "                x = Conv2DTranspose(\n",
        "                    filters = self.generator_conv_filters[i]\n",
        "                    , kernel_size = self.generator_conv_kernel_size[i]\n",
        "                    , padding = 'same'\n",
        "                    , strides = self.generator_conv_strides[i]\n",
        "                    , name = 'generator_conv_' + str(i)\n",
        "                    , kernel_initializer = self.weight_init\n",
        "                    )(x)\n",
        "\n",
        "            if i < self.n_layers_generator - 1:\n",
        "\n",
        "                if self.generator_batch_norm_momentum:\n",
        "                    x = BatchNormalization(momentum = self.generator_batch_norm_momentum)(x)\n",
        "\n",
        "                x = self.get_activation(self.generator_activation)(x)\n",
        "                \n",
        "            else:\n",
        "                x = Activation('tanh')(x)\n",
        "\n",
        "        generator_output = x\n",
        "        self.generator = Model(generator_input, generator_output)\n",
        "\n",
        "\n",
        "\n",
        "\n",
        "    def get_opti(self, lr):\n",
        "        if self.optimiser == 'adam':\n",
        "            opti = Adam(lr=lr, beta_1=0.5)\n",
        "        elif self.optimiser == 'rmsprop':\n",
        "            opti = RMSprop(lr=lr)\n",
        "        else:\n",
        "            opti = Adam(lr=lr)\n",
        "\n",
        "        return opti\n",
        "\n",
        "\n",
        "    def set_trainable(self, m, val):\n",
        "        m.trainable = val\n",
        "        for l in m.layers:\n",
        "            l.trainable = val\n",
        "\n",
        "    def _build_adversarial(self):\n",
        "                \n",
        "        #-------------------------------\n",
        "        # Construct Computational Graph\n",
        "        #       for the Critic\n",
        "        #-------------------------------\n",
        "\n",
        "        # Freeze generator's layers while training critic\n",
        "        self.set_trainable(self.generator, False)\n",
        "\n",
        "        # Image input (real sample)\n",
        "        real_img = Input(shape=self.input_dim)\n",
        "\n",
        "        # Fake image\n",
        "        z_disc = Input(shape=(self.z_dim,))\n",
        "        fake_img = self.generator(z_disc)\n",
        "\n",
        "        # critic determines validity of the real and fake images\n",
        "        fake = self.critic(fake_img)\n",
        "        valid = self.critic(real_img)\n",
        "\n",
        "        # Construct weighted average between real and fake images\n",
        "        interpolated_img = RandomWeightedAverage(self.batch_size)([real_img, fake_img])\n",
        "        # Determine validity of weighted sample\n",
        "        validity_interpolated = self.critic(interpolated_img)\n",
        "\n",
        "        # Use Python partial to provide loss function with additional\n",
        "        # 'interpolated_samples' argument\n",
        "        partial_gp_loss = partial(self.gradient_penalty_loss,\n",
        "                          interpolated_samples=interpolated_img)\n",
        "        partial_gp_loss.__name__ = 'gradient_penalty' # Keras requires function names\n",
        "\n",
        "        self.critic_model = Model(inputs=[real_img, z_disc],\n",
        "                            outputs=[valid, fake, validity_interpolated])\n",
        "\n",
        "        self.critic_model.compile(\n",
        "            loss=[self.wasserstein,self.wasserstein, partial_gp_loss]\n",
        "            ,optimizer=self.get_opti(self.critic_learning_rate)\n",
        "            ,loss_weights=[1, 1, self.grad_weight]\n",
        "            )\n",
        "        \n",
        "        #-------------------------------\n",
        "        # Construct Computational Graph\n",
        "        #         for Generator\n",
        "        #-------------------------------\n",
        "\n",
        "        # For the generator we freeze the critic's layers\n",
        "        self.set_trainable(self.critic, False)\n",
        "        self.set_trainable(self.generator, True)\n",
        "\n",
        "        # Sampled noise for input to generator\n",
        "        model_input = Input(shape=(self.z_dim,))\n",
        "        # Generate images based of noise\n",
        "        img = self.generator(model_input)\n",
        "        # Discriminator determines validity\n",
        "        model_output = self.critic(img)\n",
        "        # Defines generator model\n",
        "        self.model = Model(model_input, model_output)\n",
        "\n",
        "        self.model.compile(optimizer=self.get_opti(self.generator_learning_rate)\n",
        "        , loss=self.wasserstein\n",
        "        )\n",
        "\n",
        "        self.set_trainable(self.critic, True)\n",
        "\n",
        "    def train_critic(self, x_train, batch_size, using_generator):\n",
        "\n",
        "        valid = np.ones((batch_size,1), dtype=np.float32)\n",
        "        fake = -np.ones((batch_size,1), dtype=np.float32)\n",
        "        dummy = np.zeros((batch_size, 1), dtype=np.float32) # Dummy gt for gradient penalty\n",
        "\n",
        "        if using_generator:\n",
        "            true_imgs = next(x_train)[0]\n",
        "            if true_imgs.shape[0] != batch_size:\n",
        "                true_imgs = next(x_train)[0]\n",
        "        else:\n",
        "            idx = np.random.randint(0, x_train.shape[0], batch_size)\n",
        "            true_imgs = x_train[idx]\n",
        "    \n",
        "        noise = np.random.normal(0, 1, (batch_size, self.z_dim))\n",
        "\n",
        "        d_loss = self.critic_model.train_on_batch([true_imgs, noise], [valid, fake, dummy])\n",
        "        return d_loss\n",
        "\n",
        "    def train_generator(self, batch_size):\n",
        "        valid = np.ones((batch_size,1), dtype=np.float32)\n",
        "        noise = np.random.normal(0, 1, (batch_size, self.z_dim))\n",
        "        return self.model.train_on_batch(noise, valid)\n",
        "\n",
        "\n",
        "    def train(self, x_train, batch_size, epochs, run_folder, print_every_n_batches = 10\n",
        "    , n_critic = 5\n",
        "    , using_generator = False):\n",
        "\n",
        "        for epoch in range(self.epoch, self.epoch + epochs):\n",
        "\n",
        "            if epoch % 100 == 0:\n",
        "                critic_loops = 5\n",
        "            else:\n",
        "                critic_loops = n_critic\n",
        "\n",
        "            for _ in range(critic_loops):\n",
        "                d_loss = self.train_critic(x_train, batch_size, using_generator)\n",
        "\n",
        "            g_loss = self.train_generator(batch_size)\n",
        "\n",
        "            \n",
        "            print (\"%d (%d, %d) [D loss: (%.1f)(R %.1f, F %.1f, G %.1f)] [G loss: %.1f]\" % (epoch, critic_loops, 1, d_loss[0], d_loss[1],d_loss[2],d_loss[3],g_loss))\n",
        "            \n",
        "\n",
        "\n",
        "            self.d_losses.append(d_loss)\n",
        "            self.g_losses.append(g_loss)\n",
        "\n",
        "            # If at save interval => save generated image samples\n",
        "            if epoch % print_every_n_batches == 0:\n",
        "                self.sample_images(run_folder)\n",
        "                self.model.save_weights(os.path.join(run_folder, 'weights/weights-%d.h5' % (epoch)))\n",
        "                self.model.save_weights(os.path.join(run_folder, 'weights/weights.h5'))\n",
        "                self.save_model(run_folder)\n",
        "                \n",
        "\n",
        "            self.epoch+=1\n",
        "\n",
        "\n",
        "    def sample_images(self, run_folder):\n",
        "        r, c = 5, 5\n",
        "        noise = np.random.normal(0, 1, (r * c, self.z_dim))\n",
        "        gen_imgs = self.generator.predict(noise)\n",
        "\n",
        "        #Rescale images 0 - 1\n",
        "\n",
        "        gen_imgs = 0.5 * (gen_imgs + 1)\n",
        "        gen_imgs = np.clip(gen_imgs, 0, 1)\n",
        "\n",
        "        fig, axs = plt.subplots(r, c, figsize=(15,15))\n",
        "        cnt = 0\n",
        "\n",
        "        for i in range(r):\n",
        "            for j in range(c):\n",
        "                axs[i,j].imshow(np.squeeze(gen_imgs[cnt, :,:,:]), cmap = 'gray_r')\n",
        "                axs[i,j].axis('off')\n",
        "                cnt += 1\n",
        "        fig.savefig(os.path.join(run_folder, \"images/sample_%d.png\" % self.epoch))\n",
        "        plt.close()\n",
        "\n",
        "\n",
        "\n",
        "\n",
        "    \n",
        "    def plot_model(self, run_folder):\n",
        "        plot_model(self.model, to_file=os.path.join(run_folder ,'viz/model.png'), show_shapes = True, show_layer_names = True)\n",
        "        plot_model(self.critic, to_file=os.path.join(run_folder ,'viz/critic.png'), show_shapes = True, show_layer_names = True)\n",
        "        plot_model(self.generator, to_file=os.path.join(run_folder ,'viz/generator.png'), show_shapes = True, show_layer_names = True)\n",
        "\n",
        "\n",
        "\n",
        "            \n",
        "    def save(self, folder):\n",
        "\n",
        "            with open(os.path.join(folder, 'params.pkl'), 'wb') as f:\n",
        "                pickle.dump([\n",
        "                    self.input_dim\n",
        "                    , self.critic_conv_filters\n",
        "                    , self.critic_conv_kernel_size\n",
        "                    , self.critic_conv_strides\n",
        "                    , self.critic_batch_norm_momentum\n",
        "                    , self.critic_activation\n",
        "                    , self.critic_dropout_rate\n",
        "                    , self.critic_learning_rate\n",
        "                    , self.generator_initial_dense_layer_size\n",
        "                    , self.generator_upsample\n",
        "                    , self.generator_conv_filters\n",
        "                    , self.generator_conv_kernel_size\n",
        "                    , self.generator_conv_strides\n",
        "                    , self.generator_batch_norm_momentum\n",
        "                    , self.generator_activation\n",
        "                    , self.generator_dropout_rate\n",
        "                    , self.generator_learning_rate\n",
        "                    , self.optimiser\n",
        "                    , self.grad_weight\n",
        "                    , self.z_dim\n",
        "                    , self.batch_size\n",
        "                    ], f)\n",
        "\n",
        "            self.plot_model(folder)\n",
        "\n",
        "    def save_model(self, run_folder):\n",
        "        self.model.save(os.path.join(run_folder, 'model.h5'))\n",
        "        self.critic.save(os.path.join(run_folder, 'critic.h5'))\n",
        "        self.generator.save(os.path.join(run_folder, 'generator.h5'))\n",
        "        pickle.dump(self, open( os.path.join(run_folder, \"obj.pkl\"), \"wb\" ))\n",
        "\n",
        "    def load_weights(self, filepath):\n",
        "        self.model.load_weights(filepath)\n",
        "\n",
        "\n",
        "\n",
        "\n",
        "\n",
        "\n",
        "\n"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "WOQbdgh0tb_A",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "# ('./gdrive/My Drive/datasets/dataset_neoclassical_large_test/*.jpg')"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "7HP8Z1h0p0Nf",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "# # run params\n",
        "# SECTION = 'gan'\n",
        "# RUN_ID = '0004'\n",
        "DATA_NAME = 'zaha_final_processed/zaha_street' #/interior #move into another folder\n",
        "\n",
        "RUN_FOLDER = './gdrive/My Drive/run/zaha_street'\n",
        "# os.path.join(\"./gdrive/My Drive/datasets/dataset_neoclassical_large_test/\", data_name)\n",
        "\n",
        "\n",
        "mode =  'load' #'load' #\n",
        "\n",
        "\n",
        "\n",
        "\n",
        "\n",
        "\n",
        "\n",
        "\n",
        "\n",
        "\n",
        "\n",
        "\n",
        "\n",
        "\n"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "jp-y9rVGpsZR",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "BATCH_SIZE = 32\n",
        "IMAGE_SIZE = 224"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "4vZb0m9dp3yo",
        "colab_type": "code",
        "outputId": "78f4f9a5-9a03-4cbf-82c5-34ba6f816a23",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 34
        }
      },
      "source": [
        "x_train = load_celeb(DATA_NAME, IMAGE_SIZE, BATCH_SIZE)"
      ],
      "execution_count": 0,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Found 9183 images belonging to 1 classes.\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "9Opth3wyp5Yw",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "x_train[0][0][0]"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "2HGSFB5Op7V_",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "plt.imshow((x_train[0][0][0]+1)/2)"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "92cAYzcqp9fx",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        ""
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "5myHAL2Op-QI",
        "colab_type": "text"
      },
      "source": [
        "\n",
        "\n",
        "```\n",
        "# This is formatted as code\n",
        "```\n",
        "\n",
        "## architecture"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "SQY8SQa-qBcG",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "gan = WGANGP(input_dim = (IMAGE_SIZE,IMAGE_SIZE,3)\n",
        "        , critic_conv_filters = [64,128,256,512]\n",
        "        , critic_conv_kernel_size = [5,5,5,5]\n",
        "        , critic_conv_strides = [2,2,2,2]\n",
        "        , critic_batch_norm_momentum = None\n",
        "        , critic_activation = 'leaky_relu'\n",
        "        , critic_dropout_rate = None\n",
        "        , critic_learning_rate = 0.0002\n",
        "        , generator_initial_dense_layer_size = (14, 14, 512) # change for output size\n",
        "        , generator_upsample = [1,1,1,1]\n",
        "        , generator_conv_filters = [256,128,64,3]\n",
        "        , generator_conv_kernel_size = [5,5,5,5]\n",
        "        , generator_conv_strides = [2,2,2,2]\n",
        "        , generator_batch_norm_momentum = 0.9\n",
        "        , generator_activation = 'leaky_relu'\n",
        "        , generator_dropout_rate = None\n",
        "        , generator_learning_rate = 0.0002\n",
        "        , optimiser = 'adam'\n",
        "        , grad_weight = 10\n",
        "        , z_dim = 100\n",
        "        , batch_size = BATCH_SIZE\n",
        "        )\n",
        "\n",
        "if mode == 'build':\n",
        "    gan.save(RUN_FOLDER)\n",
        "\n",
        "else:\n",
        "    gan.load_weights(os.path.join(RUN_FOLDER, 'weights/weights.h5'))\n"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "KnTgwVBrqFCF",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "gan.critic.summary()"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "FbG6FbP6qGZS",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "gan.generator.summary()"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "37VvPN4zqHc2",
        "colab_type": "text"
      },
      "source": [
        "### training"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "jc6pfYVzqJtH",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "EPOCHS = 6000\n",
        "PRINT_EVERY_N_BATCHES = 25\n",
        "N_CRITIC = 5\n",
        "BATCH_SIZE = 32"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "YucLBMedqQDm",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "gan.train(     \n",
        "    x_train\n",
        "    , batch_size = BATCH_SIZE\n",
        "    , epochs = EPOCHS\n",
        "    , run_folder = RUN_FOLDER\n",
        "    , print_every_n_batches = PRINT_EVERY_N_BATCHES\n",
        "    , n_critic = N_CRITIC\n",
        "    , using_generator = True\n",
        ")"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "M4kWdzTlqRjq",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "fig = plt.figure()\n",
        "plt.plot([x[0] for x in gan.d_losses], color='black', linewidth=0.25)\n",
        "\n",
        "plt.plot([x[1] for x in gan.d_losses], color='green', linewidth=0.25)\n",
        "plt.plot([x[2] for x in gan.d_losses], color='red', linewidth=0.25)\n",
        "plt.plot(gan.g_losses, color='orange', linewidth=0.25)\n",
        "\n",
        "plt.xlabel('batch', fontsize=18)\n",
        "plt.ylabel('loss', fontsize=16)\n",
        "\n",
        "plt.xlim(0, 2000)\n",
        "# plt.ylim(0, 2)\n",
        "\n",
        "plt.show()"
      ],
      "execution_count": 0,
      "outputs": []
    }
  ]
}